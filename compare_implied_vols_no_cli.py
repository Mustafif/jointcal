#!/usr/bin/env python3
"""
Compare implied vol surfaces generated by different parameter sets
defined in mc_moments_all_datasets.json.

This version uses a Monte Carlo simulation approach to test parameter sensitivity
to noise (0%, 1%, 3%, 5%) and compares the four moments (Mean, Std, Skew, Kurtosis)
of the resulting volatility surfaces against the true parameters.
"""
import json
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import torch
from scipy.stats import kurtosis, skew
from sklearn.preprocessing import MinMaxScaler

from model import IV_GLU

# Hardcoded configuration - modify these as needed
JSON_PATH = "calibration_results/mc_moments_all_datasets.json"
MODEL_PATH = "saved_models/scalable_hn_dataset_250x60_20250803/model.pt"
REFERENCE_PARAM_SET = "true_parameters"
DEVICE = "cuda"

# Monte Carlo Configuration
NOISE_LEVELS = [0.0, 0.01, 0.03, 0.05]  # 0%, 1%, 3%, 5%
MC_PATHS = 50  # Number of paths for simulation
OUTPUT_JSON_PATH = "calibration_results/iv_moments_analysis.json"

BASE_FEATURES = ["S0", "m", "r", "T", "corp",
                 "alpha", "beta", "omega", "gamma", "lambda", "V"]
ENGINEERED_FEATURES = [
    "strike", "returns", "log_moneyness", "moneyness_squared",
    "sqrt_T", "log_T", "inv_T", "time_decay",
    "log_gamma", "log_omega", "log_lambda", "log_alpha", "log_beta",
    "risk_free_T", "m_T_interaction", "value_ratio", "log_value",
    "is_call", "corp_m_interaction", "corp_T_interaction",
    "moneyness_centered", "atm_indicator", "alpha_beta", "alpha_gamma",
    "beta_squared", "persistence", "mean_reversion"
]
ALL_FEATURES = BASE_FEATURES + ENGINEERED_FEATURES


def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:
    eps = 1e-8
    features = df.copy()
    features["strike"] = features["S0"] * features["m"]
    features["returns"] = 0.0
    features["log_moneyness"] = np.log(features["m"] + eps)
    features["moneyness_squared"] = features["m"] ** 2
    features["sqrt_T"] = np.sqrt(features["T"] + eps)
    features["log_T"] = np.log(features["T"] + eps)
    features["inv_T"] = 1.0 / (features["T"] + eps)
    features["time_decay"] = np.exp(-features["T"])
    features["log_gamma"] = np.log(features["gamma"] + eps)
    features["log_omega"] = np.log(features["omega"] + eps)
    features["log_alpha"] = np.log(features["alpha"] + eps)
    features["log_beta"] = np.log(features["beta"] + eps)
    features["log_lambda"] = np.log(features["lambda"] + eps)
    features["risk_free_T"] = features["r"] * features["T"]
    features["m_T_interaction"] = features["m"] * features["T"]
    features["value_ratio"] = features["V"] / (features["S0"] + eps)
    features["log_value"] = np.log(features["V"] + eps)
    features["is_call"] = (features["corp"] + 1) / 2
    features["corp_m_interaction"] = features["corp"] * features["m"]
    features["corp_T_interaction"] = features["corp"] * features["T"]

    # Additional features needed for the model
    features["moneyness_centered"] = features["m"] - 1.0
    features["atm_indicator"] = np.exp(-10 * (features["m"] - 1.0) ** 2)
    features["alpha_beta"] = features["alpha"] * features["beta"]
    features["alpha_gamma"] = features["alpha"] * features["gamma"]
    features["beta_squared"] = features["beta"] ** 2
    features["persistence"] = features["alpha"] + features["beta"]
    features["mean_reversion"] = 1.0 - features["persistence"]

    return features


def build_feature_tensor(df: pd.DataFrame) -> torch.Tensor:
    engineered = add_engineered_features(df)
    return torch.tensor(engineered[ALL_FEATURES].values, dtype=torch.float32)


def load_iv_model(model_path: Path, device: torch.device) -> torch.nn.Module:
    return torch.load(model_path, map_location=device, weights_only=False)


def predict_sigmas(model, features, scaler, device):
    with torch.no_grad():
        preds_scaled = model(features.to(device)).cpu().numpy()
    preds = scaler.inverse_transform(preds_scaled)
    return preds.flatten()


def calculate_moments(vols: np.ndarray) -> Dict[str, float]:
    """Calculate the four moments of the volatility surface."""
    return {
        "mean": float(np.mean(vols)),
        "std": float(np.std(vols, ddof=1)),
        "skew": float(skew(vols)),
        "kurtosis": float(kurtosis(vols))
    }


def main():
    print("=" * 100)
    print("IMPLIED VOLATILITY MOMENTS ANALYSIS (MONTE CARLO)")
    print("=" * 100)
    print(f"JSON File: {JSON_PATH}")
    print(f"Model: {MODEL_PATH}")
    print(f"Reference Parameter Set: {REFERENCE_PARAM_SET}")
    print(f"Device: {DEVICE}")
    print(f"Noise Levels: {[f'{n:.0%}' for n in NOISE_LEVELS]}")
    print(f"MC Paths: {MC_PATHS}")
    print()

    # Load JSON data
    json_path = Path(JSON_PATH)
    data = json.loads(json_path.read_text())
    print(f"Loaded {len(data)} datasets from JSON file")

    # Set up device
    device = torch.device(DEVICE if DEVICE == "cuda" and torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load model
    model = load_iv_model(Path(MODEL_PATH), device)
    print("Model loaded successfully")

    all_results = {}

    # Process each dataset
    for dataset_key, dataset_entry in data.items():
        dataset_info = dataset_entry["dataset_info"]
        # Handle relative paths
        dataset_path = Path(dataset_info["dataset_path"])
        if not dataset_path.is_absolute() and not dataset_path.exists():
            dataset_path = Path.cwd() / dataset_info["dataset_path"]
        if not dataset_path.exists():
            print(f"Warning: Dataset file not found: {dataset_path}")
            continue

        df_raw = pd.read_csv(dataset_path)
        scaler = MinMaxScaler().fit(df_raw[["sigma"]].values)

        print("\n" + "="*100)
        print(f"DATASET: {dataset_key}")
        print(f"Path: {dataset_info['dataset_path']}")
        print(f"Rows: {len(df_raw):,}")
        print("-" * 100)

        dataset_results = {
            "dataset_info": dataset_info,
            "baseline_moments": None,
            "parameter_sets": {}
        }

        # First, establish the baseline moments from the reference set (0% noise)
        baseline_moments = None
        if REFERENCE_PARAM_SET in dataset_entry["parameter_sets"]:
            print(f"Calculating baseline from {REFERENCE_PARAM_SET}...")
            ref_params = dataset_entry["parameter_sets"][REFERENCE_PARAM_SET]["parameters"]
            df_ref = df_raw.copy()
            for param_name, value in ref_params.items():
                if param_name in df_ref.columns:
                    df_ref[param_name] = value

            features_ref = build_feature_tensor(df_ref)
            vols_ref = predict_sigmas(model, features_ref, scaler, device)
            baseline_moments = calculate_moments(vols_ref)
            dataset_results["baseline_moments"] = baseline_moments

            print(f"BASELINE ({REFERENCE_PARAM_SET}):")
            print(f"  Mean: {baseline_moments['mean']:.4f} | Std: {baseline_moments['std']:.4f} | "
                  f"Skew: {baseline_moments['skew']:.4f} | Kurt: {baseline_moments['kurtosis']:.4f}")
            print("-" * 100)
        else:
            print(f"Warning: Reference parameter set '{REFERENCE_PARAM_SET}' not found in dataset.")

        # Process each parameter set in the dataset
        for name, param_entry in dataset_entry["parameter_sets"].items():
            print(f"\nParameter Set: {name}")
            base_params = param_entry["parameters"]

            # Display base parameters
            param_str = " | ".join([f"{k}={v}" if isinstance(v, (int, float)) and v < 1e-3
                                   else f"{k}={v:.3f}" if isinstance(v, float)
                                   else f"{k}={v}" for k, v in base_params.items()])
            print(f"  Base Params: {param_str}")

            param_set_results = {
                "base_parameters": base_params,
                "noise_analysis": {}
            }

            # Header for results
            print(f"  {'Noise':<8} | {'Mean':<15} | {'Std':<15} | {'Skew':<15} | {'Kurtosis':<15}")
            print(f"  {'-'*8} | {'-'*15} | {'-'*15} | {'-'*15} | {'-'*15}")

            # Run simulations for each noise level
            for noise in NOISE_LEVELS:
                path_moments = {
                    "mean": [], "std": [], "skew": [], "kurtosis": []
                }

                # Determine number of paths (1 for 0% noise, MC_PATHS for others)
                n_paths = 1 if noise == 0.0 else MC_PATHS

                for _ in range(n_paths):
                    df = df_raw.copy()

                    # Apply parameters with noise
                    for param_name, value in base_params.items():
                        if noise > 0:
                            perturbation = np.random.normal(0, noise)
                            perturbed_value = value * (1 + perturbation)
                            # Keep positive parameters positive
                            if value > 0 and perturbed_value < 0:
                                perturbed_value = abs(perturbed_value)
                        else:
                            perturbed_value = value

                        if param_name in df.columns:
                            df[param_name] = perturbed_value

                    features = build_feature_tensor(df)
                    vols = predict_sigmas(model, features, scaler, device)

                    m = calculate_moments(vols)
                    for k in path_moments:
                        path_moments[k].append(m[k])

                # Average moments across paths
                avg_moments = {k: float(np.mean(v)) for k, v in path_moments.items()}
                param_set_results["noise_analysis"][str(noise)] = avg_moments

                # Format output with deltas if baseline exists
                def fmt_val(key):
                    val = avg_moments[key]
                    if baseline_moments:
                        delta = val - baseline_moments[key]
                        return f"{val:.4f} ({delta:+.4f})"
                    return f"{val:.4f}"

                print(f"  {noise:<8.0%} | {fmt_val('mean'):<15} | {fmt_val('std'):<15} | "
                      f"{fmt_val('skew'):<15} | {fmt_val('kurtosis'):<15}")

            dataset_results["parameter_sets"][name] = param_set_results

        all_results[dataset_key] = dataset_results

    # Save results to JSON
    output_path = Path(OUTPUT_JSON_PATH)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(all_results, f, indent=2)
    print(f"\nResults saved to {output_path}")

    print("\n" + "="*100)
    print("ANALYSIS COMPLETE")
    print("="*100)


if __name__ == "__main__":
    main()
